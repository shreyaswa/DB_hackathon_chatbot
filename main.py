import streamlit as st
import requests # Used for making HTTP requests to the Ollama API
import json
import os
from dotenv import load_dotenv # Import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
# Ollama API endpoint (default for local Ollama server)
OLLAMA_API_BASE = os.getenv("OLLAMA_API_BASE")
# Choose the model you have pulled in Ollama (e.g., 'llama2', 'mistral', 'gemma', 'llava' for multimodal)
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL")

# --- Backend Prompt Placeholders ---
DEFAULT_SYSTEM_PROMPT = "You are a helpful and friendly AI assistant. Keep your responses concise and relevant to the provided information."

# Load the content of prompt.txt into FILE_ANALYSIS_TRIGGER_PROMPT
with open("prompt.txt", "r", encoding="utf-8") as file:
    FILE_ANALYSIS_TRIGGER_PROMPT = file.read()

# --- Modular Functions ---

def initialize_session_state():
    """Initializes Streamlit session state variables."""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "uploaded_file_content" not in st.session_state:
        st.session_state.uploaded_file_content = None
    if "uploaded_file_name" not in st.session_state:
        st.session_state.uploaded_file_name = None
    # Flag to indicate if the initial file analysis prompt has been triggered
    if "file_analysis_triggered" not in st.session_state:
        st.session_state.file_analysis_triggered = False

def display_chat_history():
    """Displays existing chat messages from session state."""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def get_ollama_response(user_prompt: str, chat_history: list, system_prompt: str = DEFAULT_SYSTEM_PROMPT, file_content: str = None) -> str:
    """
    Sends the chat history and optional file content to Ollama and streams the response.

    Args:
        user_prompt: The latest message from the user.
        chat_history: The list of previous messages in the conversation.
        system_prompt: The system prompt to guide the LLM's behavior.
        file_content: Optional content from an uploaded file to be included in the prompt.

    Returns:
        The full response generated by the Ollama model.
    """
    full_response = ""
    message_placeholder = st.empty() # Placeholder for streaming response

    try:
        # Prepare messages for Ollama API, starting with the system prompt
        ollama_messages = [
            {"role": "system", "content": system_prompt}
        ]

        # Add file content as a user message if available
        if file_content:
            # It's crucial to include the file content *before* the current chat history
            # so the model has context for the entire conversation.
            ollama_messages.append({"role": "user", "content": f"The following is content from a file named '{st.session_state.uploaded_file_name}':\n\n```\n{file_content}\n```\n\nPlease analyze this content in the context of our conversation."})

        # Add existing chat history to maintain context
        # Filter out any file processing messages that are just for UI display
        filtered_chat_history = [
            msg for msg in chat_history
            if not msg["content"].startswith("Processing file:") and not msg["content"].startswith("File content:")
        ]
        ollama_messages.extend([{"role": m["role"], "content": m["content"]} for m in filtered_chat_history])

        # Add the current user prompt (or trigger prompt)
        ollama_messages.append({"role": "user", "content": user_prompt})

        payload = {
            "model": OLLAMA_MODEL,
            "messages": ollama_messages,
            "stream": True
        }

        with requests.post(f"{OLLAMA_API_BASE}/api/chat", json=payload, stream=True) as response:
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line.decode('utf-8'))
                        if "message" in chunk and "content" in chunk["message"]:
                            content_chunk = chunk["message"]["content"]
                            full_response += content_chunk
                            message_placeholder.markdown(full_response + "â–Œ") # Add blinking cursor
                    except json.JSONDecodeError:
                        continue # Skip incomplete JSON lines
            message_placeholder.markdown(full_response) # Display final response

    except requests.exceptions.ConnectionError:
        st.error(f"Could not connect to Ollama server at {OLLAMA_API_BASE}. Please ensure Ollama is running and the model '{OLLAMA_MODEL}' is pulled.")
        full_response = "I'm sorry, I can't connect to the local LLM. Please check if Ollama is running."
        message_placeholder.markdown(full_response)
    except requests.exceptions.HTTPError as e:
        st.error(f"Ollama API Error: {e.response.status_code} - {e.response.text}")
        full_response = "I'm sorry, I encountered an error with the Ollama API. Please check the server logs."
        message_placeholder.markdown(full_response)
    except Exception as e:
        st.error(f"An unexpected error occurred: {e}")
        full_response = "An unexpected error occurred. Please check your Ollama setup."
        message_placeholder.markdown(full_response)

    return full_response

# --- Main Application Logic ---
def main():
    """Main function to run the Streamlit chatbot application."""
    # Ensure environment variables are set
    if not OLLAMA_API_BASE:
        st.error("OLLAMA_API_BASE environment variable is not set. Please create a .env file or set the variable.")
        st.stop()
    if not OLLAMA_MODEL:
        st.error("OLLAMA_MODEL environment variable is not set. Please create a .env file or set the variable.")
        st.stop()

    # Streamlit UI Setup
    st.set_page_config(page_title=f"Simple {OLLAMA_MODEL.capitalize()} Chatbot", layout="centered")
    st.title(f"ðŸ¤– Simple {OLLAMA_MODEL.capitalize()} Chatbot (via Ollama)")
    st.markdown("---")

    initialize_session_state()
    display_chat_history()

    # File Uploader
    uploaded_file = st.file_uploader("Upload a document for the LLM to analyze (e.g., .txt, .md)", type=["txt", "md"])

    current_file_content = None
    if uploaded_file is not None:
        # Check if a new file has been uploaded or if it's a different file
        if st.session_state.uploaded_file_name != uploaded_file.name:
            st.session_state.uploaded_file_name = uploaded_file.name
            st.session_state.uploaded_file_content = uploaded_file.read().decode("utf-8")
            st.session_state.file_analysis_triggered = False # Reset trigger for new file
            st.session_state.messages.append({"role": "assistant", "content": f"File '{uploaded_file.name}' uploaded successfully. Analyzing content..."})
            st.rerun() # Rerun to display the file upload confirmation message and trigger analysis
        current_file_content = st.session_state.uploaded_file_content

        # Trigger initial analysis if a new file is uploaded and not yet analyzed
        if current_file_content and not st.session_state.file_analysis_triggered:
            with st.chat_message("assistant"):
                # Use a temporary placeholder for the AI's thinking process
                thinking_placeholder = st.empty()
                thinking_placeholder.markdown("Thinking about the file content...")

                # Get the initial analysis response from Ollama
                initial_analysis_response = get_ollama_response(
                    user_prompt=FILE_ANALYSIS_TRIGGER_PROMPT, # Use the specific trigger prompt
                    chat_history=[], # Start fresh for this initial analysis, but pass file_content
                    file_content=current_file_content
                )
                # Remove thinking placeholder and display actual response
                thinking_placeholder.empty()
                st.session_state.messages.append({"role": "assistant", "content": initial_analysis_response})
                st.session_state.file_analysis_triggered = True # Mark analysis as triggered
                st.rerun() # Rerun to display the initial analysis and clear thinking message

    # Chat Input and Response Generation
    if prompt := st.chat_input("What's on your mind?"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Get response from Ollama, passing file content if available
        # The file content is now handled by the initial trigger, but can be passed again
        # if the model needs to re-reference it explicitly in later turns.
        # For simplicity, we'll let the chat history carry the context after initial analysis.
        assistant_response = get_ollama_response(prompt, st.session_state.messages, file_content=None) # File content already processed

        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})

    st.markdown("---")
    st.info(f"This chatbot uses the '{OLLAMA_MODEL}' model via Ollama. Ensure Ollama is running and the model is downloaded.")

if __name__ == "__main__":
    main()
